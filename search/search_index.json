{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\udcca Welcome to Loyalytics Documentation Loyalytics is a marketing analytics platform developed for a supermarket chain's Bonus Card Loyalty Program . The project addresses declining sales by analyzing customer behavior, segmenting loyalty profiles, and enabling data-driven decision-making. \ud83e\udde0 Problem Despite having a bonus card program, the supermarket has experienced declining sales. The challenge is identifying: \"What factors are contributing to the drop in sales among bonus cardholders, and how can segmentation improve retention?\" \u2705 Solution We implemented an end-to-end analytics platform that: Extracts, cleans, and loads transactional and customer data (ETL). Builds a star schema database to support analytical workloads. Performs RFM segmentation and survival analysis on customer data. Visualizes insights via a Streamlit dashboard. Deploys services using Docker Compose for full-stack orchestration. \ud83c\udfaf Expected Outcomes Clear understanding of customer loyalty patterns. Identification of high-value vs. at-risk segments. Actionable insights through survival modeling and segment summaries. A fully reproducible and documented system accessible via: Streamlit UI: localhost:8501 FastAPI Docs: localhost:8008/docs PgAdmin UI: localhost:5050 GitHub Pages: https://ds-223.github.io/Group-5/ \ud83d\udcc1 Documentation Map database.md \u2013 Database schema, ORM models, and raw table loading model.md \u2013 ML logic, RFM pipeline, and survival analysis api.md \u2013 RESTful API endpoints for predictions and search","title":"Home"},{"location":"#welcome-to-loyalytics-documentation","text":"Loyalytics is a marketing analytics platform developed for a supermarket chain's Bonus Card Loyalty Program . The project addresses declining sales by analyzing customer behavior, segmenting loyalty profiles, and enabling data-driven decision-making.","title":"\ud83d\udcca Welcome to Loyalytics Documentation"},{"location":"#problem","text":"Despite having a bonus card program, the supermarket has experienced declining sales. The challenge is identifying: \"What factors are contributing to the drop in sales among bonus cardholders, and how can segmentation improve retention?\"","title":"\ud83e\udde0 Problem"},{"location":"#solution","text":"We implemented an end-to-end analytics platform that: Extracts, cleans, and loads transactional and customer data (ETL). Builds a star schema database to support analytical workloads. Performs RFM segmentation and survival analysis on customer data. Visualizes insights via a Streamlit dashboard. Deploys services using Docker Compose for full-stack orchestration.","title":"\u2705 Solution"},{"location":"#expected-outcomes","text":"Clear understanding of customer loyalty patterns. Identification of high-value vs. at-risk segments. Actionable insights through survival modeling and segment summaries. A fully reproducible and documented system accessible via: Streamlit UI: localhost:8501 FastAPI Docs: localhost:8008/docs PgAdmin UI: localhost:5050 GitHub Pages: https://ds-223.github.io/Group-5/","title":"\ud83c\udfaf Expected Outcomes"},{"location":"#documentation-map","text":"database.md \u2013 Database schema, ORM models, and raw table loading model.md \u2013 ML logic, RFM pipeline, and survival analysis api.md \u2013 RESTful API endpoints for predictions and search","title":"\ud83d\udcc1 Documentation Map"},{"location":"api/","text":"\ud83d\ude80 API Documentation This service exposes the REST API layer for the Customer Loyalty & Analytics system. It powers the dashboard, segmentation visualizations, survival models, and automated email campaigns. Built with FastAPI , the API provides endpoints for querying customer data, analyzing transactions, generating RFM segments, and sending retention emails. \u2699\ufe0f Tech Stack FastAPI \u2013 Lightweight, fast web framework for REST endpoints SQLAlchemy \u2013 ORM for interacting with PostgreSQL Pydantic \u2013 Request validation & API response schemas Lifelines \u2013 Kaplan-Meier survival analysis BackgroundTasks \u2013 Async email dispatching \ud83d\udc64 Customer Endpoints Manage customer profiles stored in DimCustomer : POST /customers/ \u2013 Create a new customer GET /customers/{id} \u2013 Retrieve customer details PUT /customers/{id} \u2013 Update customer profile DELETE /customers/{id} \u2013 Remove a customer record Also includes: GET /analytics/customer-count-by-gender \u2013 Gender-wise demographic distribution \ud83d\udcb0 Transaction & Revenue Analytics Explore sales data using joins over the star schema: GET /revenue/monthly \u2013 Monthly revenue totals GET /customers/{id}/transactions \u2013 Full transaction history of a customer GET /analytics/transactions-by-store-month \u2013 Monthly revenue by store GET /analytics/transaction-amount-by-store \u2013 Lifetime revenue by store \ud83d\udcca RFM Segmentation Provides data on Recency-Frequency-Monetary segments : GET /analytics/customers-by-segment/{segment} \u2013 List of customers in a specific segment GET /analytics/segment-distribution/{all|male|female} \u2013 Distribution of segments overall or by gender GET /analytics/rfm-matrix \u2013 RFM matrix for segment summaries \ud83d\udce7 Email Campaigns Launch targeted retention campaigns via segment-specific email templates: GET /analytics/segments_for_button \u2013 List of all segments for frontend dropdowns POST /campaigns/{segment} \u2013 Asynchronously send emails to all users in a segment using BackgroundTasks Each template includes personalized subject lines and discount codes. See logic in EmailCampaignManager . \ud83d\udcc8 Survival Analysis Use Kaplan-Meier to visualize customer churn over time: GET /analytics/survival-curve \u2013 Returns survival probability at each time step using SurvivalData Supports visualization of customer retention lifecycle. \ud83d\udee0\ufe0f Example Schema: CustomerCreate { \"CustomerKey\": 123, \"CustomerCardCode\": \"BNS1234567890\", \"Name\": \"Jane Doe\", \"RegistrationDate\": \"2020-01-01T00:00:00\", \"BirthDate\": \"1990-05-10\", \"Gender\": \"Female\", \"Phone\": \"(555) 123-4567\", \"Address\": \"123 Loyalty St.\", \"Email\": \"jane.doe@example.com\" } \ud83d\udd10 Security & Config Sensitive credentials like DB URLs or Gmail credentials are stored in a .env file. Email logic uses OAuth-friendly app passwords. \ud83d\udcc2 Related Files File Description main.py FastAPI app with all endpoints schema.py Pydantic models for request/response email_utils.py Email template logic and SMTP sending columns.py SQLAlchemy table mappings Dockerfile FastAPI container setup requirements.txt All dependencies (e.g., lifelines, fastapi, sqlalchemy) \ud83e\uddea Testing Test endpoints locally using: Swagger UI: localhost:8000/docs ReDoc: localhost:8000/redoc \u2705 Status \ud83d\udcec Email logic tested for all segments \ud83d\udcc8 Survival curve rendered correctly \u2705 Endpoints verified via Swagger \ud83d\udc33 Works with Docker Compose stack For more information, explore index.md , database.md , and model.md .","title":"API"},{"location":"api/#api-documentation","text":"This service exposes the REST API layer for the Customer Loyalty & Analytics system. It powers the dashboard, segmentation visualizations, survival models, and automated email campaigns. Built with FastAPI , the API provides endpoints for querying customer data, analyzing transactions, generating RFM segments, and sending retention emails.","title":"\ud83d\ude80 API Documentation"},{"location":"api/#tech-stack","text":"FastAPI \u2013 Lightweight, fast web framework for REST endpoints SQLAlchemy \u2013 ORM for interacting with PostgreSQL Pydantic \u2013 Request validation & API response schemas Lifelines \u2013 Kaplan-Meier survival analysis BackgroundTasks \u2013 Async email dispatching","title":"\u2699\ufe0f Tech Stack"},{"location":"api/#customer-endpoints","text":"Manage customer profiles stored in DimCustomer : POST /customers/ \u2013 Create a new customer GET /customers/{id} \u2013 Retrieve customer details PUT /customers/{id} \u2013 Update customer profile DELETE /customers/{id} \u2013 Remove a customer record Also includes: GET /analytics/customer-count-by-gender \u2013 Gender-wise demographic distribution","title":"\ud83d\udc64 Customer Endpoints"},{"location":"api/#transaction-revenue-analytics","text":"Explore sales data using joins over the star schema: GET /revenue/monthly \u2013 Monthly revenue totals GET /customers/{id}/transactions \u2013 Full transaction history of a customer GET /analytics/transactions-by-store-month \u2013 Monthly revenue by store GET /analytics/transaction-amount-by-store \u2013 Lifetime revenue by store","title":"\ud83d\udcb0 Transaction &amp; Revenue Analytics"},{"location":"api/#rfm-segmentation","text":"Provides data on Recency-Frequency-Monetary segments : GET /analytics/customers-by-segment/{segment} \u2013 List of customers in a specific segment GET /analytics/segment-distribution/{all|male|female} \u2013 Distribution of segments overall or by gender GET /analytics/rfm-matrix \u2013 RFM matrix for segment summaries","title":"\ud83d\udcca RFM Segmentation"},{"location":"api/#email-campaigns","text":"Launch targeted retention campaigns via segment-specific email templates: GET /analytics/segments_for_button \u2013 List of all segments for frontend dropdowns POST /campaigns/{segment} \u2013 Asynchronously send emails to all users in a segment using BackgroundTasks Each template includes personalized subject lines and discount codes. See logic in EmailCampaignManager .","title":"\ud83d\udce7 Email Campaigns"},{"location":"api/#survival-analysis","text":"Use Kaplan-Meier to visualize customer churn over time: GET /analytics/survival-curve \u2013 Returns survival probability at each time step using SurvivalData Supports visualization of customer retention lifecycle.","title":"\ud83d\udcc8 Survival Analysis"},{"location":"api/#example-schema-customercreate","text":"{ \"CustomerKey\": 123, \"CustomerCardCode\": \"BNS1234567890\", \"Name\": \"Jane Doe\", \"RegistrationDate\": \"2020-01-01T00:00:00\", \"BirthDate\": \"1990-05-10\", \"Gender\": \"Female\", \"Phone\": \"(555) 123-4567\", \"Address\": \"123 Loyalty St.\", \"Email\": \"jane.doe@example.com\" }","title":"\ud83d\udee0\ufe0f Example Schema: CustomerCreate"},{"location":"api/#security-config","text":"Sensitive credentials like DB URLs or Gmail credentials are stored in a .env file. Email logic uses OAuth-friendly app passwords.","title":"\ud83d\udd10 Security &amp; Config"},{"location":"api/#related-files","text":"File Description main.py FastAPI app with all endpoints schema.py Pydantic models for request/response email_utils.py Email template logic and SMTP sending columns.py SQLAlchemy table mappings Dockerfile FastAPI container setup requirements.txt All dependencies (e.g., lifelines, fastapi, sqlalchemy)","title":"\ud83d\udcc2 Related Files"},{"location":"api/#testing","text":"Test endpoints locally using: Swagger UI: localhost:8000/docs ReDoc: localhost:8000/redoc","title":"\ud83e\uddea Testing"},{"location":"api/#status","text":"\ud83d\udcec Email logic tested for all segments \ud83d\udcc8 Survival curve rendered correctly \u2705 Endpoints verified via Swagger \ud83d\udc33 Works with Docker Compose stack For more information, explore index.md , database.md , and model.md .","title":"\u2705 Status"},{"location":"app/","text":"","title":"App"},{"location":"database/","text":"Database Documentation This service manages the PostgreSQL database for the ETL pipeline and analytics platform. It uses SQLAlchemy ORM to define and interact with the star schema that powers the reporting and machine learning workflows. \ud83d\udcd0 Star Schema Design The schema follows a classic star structure with the following tables: DimDate : Stores date-related attributes. DimCustomer : Contains customer metadata (e.g., name, birthdate, gender, email). DimStore : Information about store branches including location and size. FactTransaction : The central fact table that records every customer transaction. Each FactTransaction entry references: a customer ( CustomerKey from DimCustomer ), a store ( StoreKey from DimStore ), and a transaction date ( DateKey from DimDate ). \u2699\ufe0f ORM Models The ORM classes in db/star_schema.py define the schema as Python classes. All models inherit from a common Base defined in db_conf.py . The create_tables() function initializes the schema in the target PostgreSQL database. Base.metadata.create_all(bind=engine) \ud83e\udde0 Business Logic Layer The class TransactionDatabase in CRUD_func.py wraps direct database interactions such as: Inserting customers, dates, and transactions Updating or deleting records Fetching data for reporting or debugging \ud83d\udd01 ETL Integration The ETL pipeline loads data into the star schema in multiple phases: Extract & Load Raw XLSX : Excel files are ingested into raw tables using Pandas and to_sql() . Raw file names are cleaned into table names. Transform : Data is cleaned and standardized (e.g., phone/email formatting, gender unification). Invalid records are dropped. Load to Star Schema : Dimension tables are filled using load_dimcustomer_table() and load_dimdate_table() . Transaction data is processed and inserted using load_facttransaction_table() . \ud83e\uddea Data Validations Dates must follow YYYY-MM-DD and be logically consistent. CustomerCardCodes are filtered to 13-character strings. Phone and address fields are validated for structure and length. Store names are mapped to known clean values using STORE_NAME_MAPPING . \ud83d\udee0\ufe0f Dependencies The ETL database logic requires: SQLAlchemy psycopg2-binary pandas loguru dotenv \ud83d\udd10 Configuration Database credentials are loaded from a .env file: DATABASE_URL=postgresql+psycopg2://postgres:postgres@db:5432/maindb DB_USER=postgres DB_HOST=postgres-db DB_PASSWORD=postgres DB_NAME=maindb PGADMIN_EMAIL=admin@admin.com PGADMIN_PASSWORD=admin \ud83d\udcc2 Related Files File Description /db SQLAlchemy Setup: Engine, Base, Session, General Database Configuration CRUD_func.py Class wrapping SQL queries extract_load_raw.py Extracts all XLSX files, does validation and dumps them in the DB transform.py Takes raw tables and does transformation, cleaning and validation load.py Takes clean tables and sequentially loads into DB, row-by-row (FactTransaction, DimCustomer) or by batch main.py Entry point that orchestrates ETL and signals completion","title":"Database"},{"location":"database/#database-documentation","text":"This service manages the PostgreSQL database for the ETL pipeline and analytics platform. It uses SQLAlchemy ORM to define and interact with the star schema that powers the reporting and machine learning workflows.","title":"Database Documentation"},{"location":"database/#star-schema-design","text":"The schema follows a classic star structure with the following tables: DimDate : Stores date-related attributes. DimCustomer : Contains customer metadata (e.g., name, birthdate, gender, email). DimStore : Information about store branches including location and size. FactTransaction : The central fact table that records every customer transaction. Each FactTransaction entry references: a customer ( CustomerKey from DimCustomer ), a store ( StoreKey from DimStore ), and a transaction date ( DateKey from DimDate ).","title":"\ud83d\udcd0 Star Schema Design"},{"location":"database/#orm-models","text":"The ORM classes in db/star_schema.py define the schema as Python classes. All models inherit from a common Base defined in db_conf.py . The create_tables() function initializes the schema in the target PostgreSQL database. Base.metadata.create_all(bind=engine)","title":"\u2699\ufe0f ORM Models"},{"location":"database/#business-logic-layer","text":"The class TransactionDatabase in CRUD_func.py wraps direct database interactions such as: Inserting customers, dates, and transactions Updating or deleting records Fetching data for reporting or debugging","title":"\ud83e\udde0 Business Logic Layer"},{"location":"database/#etl-integration","text":"The ETL pipeline loads data into the star schema in multiple phases: Extract & Load Raw XLSX : Excel files are ingested into raw tables using Pandas and to_sql() . Raw file names are cleaned into table names. Transform : Data is cleaned and standardized (e.g., phone/email formatting, gender unification). Invalid records are dropped. Load to Star Schema : Dimension tables are filled using load_dimcustomer_table() and load_dimdate_table() . Transaction data is processed and inserted using load_facttransaction_table() .","title":"\ud83d\udd01 ETL Integration"},{"location":"database/#data-validations","text":"Dates must follow YYYY-MM-DD and be logically consistent. CustomerCardCodes are filtered to 13-character strings. Phone and address fields are validated for structure and length. Store names are mapped to known clean values using STORE_NAME_MAPPING .","title":"\ud83e\uddea Data Validations"},{"location":"database/#dependencies","text":"The ETL database logic requires: SQLAlchemy psycopg2-binary pandas loguru dotenv","title":"\ud83d\udee0\ufe0f Dependencies"},{"location":"database/#configuration","text":"Database credentials are loaded from a .env file: DATABASE_URL=postgresql+psycopg2://postgres:postgres@db:5432/maindb DB_USER=postgres DB_HOST=postgres-db DB_PASSWORD=postgres DB_NAME=maindb PGADMIN_EMAIL=admin@admin.com PGADMIN_PASSWORD=admin","title":"\ud83d\udd10 Configuration"},{"location":"database/#related-files","text":"File Description /db SQLAlchemy Setup: Engine, Base, Session, General Database Configuration CRUD_func.py Class wrapping SQL queries extract_load_raw.py Extracts all XLSX files, does validation and dumps them in the DB transform.py Takes raw tables and does transformation, cleaning and validation load.py Takes clean tables and sequentially loads into DB, row-by-row (FactTransaction, DimCustomer) or by batch main.py Entry point that orchestrates ETL and signals completion","title":"\ud83d\udcc2 Related Files"},{"location":"model/","text":"Model Service The ds service is responsible for performing advanced data analytics, including customer segmentation (via RFM analysis) and customer retention modeling (via survival analysis). It uses preprocessed transaction and customer data from the ETL pipeline and produces insights that are stored in the database and visualized in the frontend. \ud83e\uddf1 Architecture Overview The model pipeline is built with: SQLAlchemy & pandas : For database interaction and data manipulation scikit-learn : For segmentation (KNN for unknown segment classification) lifelines : For survival analysis (Cox Proportional Hazards, Weibull AFT) matplotlib : For visualization Python + Docker : For containerized execution \ud83d\udce6 Input & Output Files File Path Purpose outputs/customer_transactions.csv Extracted customer transaction data outputs/rfm_results.csv Output of the RFM analysis outputs/survival_data.csv Prepared survival data outputs/*.png Survival plots outputs/*.csv Model summaries \ud83d\udd0d Components 1. Data Extraction Module: db_ops/extract_and_save.py extract_transaction_data() : Joins FactTransaction, DimCustomer, and DimDate tables for RFM analysis. extract_survival_data() : Constructs a survival dataset with event and duration variables. 2. RFM Analysis Module: utils/rfm_analyzer.py calculate_rfm() : Computes recency, frequency, and monetary values per customer. score_rfm() : Assigns 1\u20135 scores for each metric. segment_customers() : Classifies customers into segments such as: Champions Loyal Customers Potential Loyalists Big Spenders Leaving Customers classify_unknown_segments() : Uses KNN classifier to assign a segment to previously unclassified customers. analyze_segments() : Produces aggregated statistics per segment. save_results() : Saves the detailed results to CSV. 3. Survival Analysis Module: utils/survival_analyzer.py Uses lifelines to fit: Cox Proportional Hazards Model Weibull AFT Model Key Methods: fit_cox_model() fit_weibull_model() print_model_summaries() save_model_summaries() plot_weibull_survival_function() plot_custom_profiles() \ud83e\uddea Output Tables in Database Table Description RFMResults RFM scores, segments, and demographic details SurvivalData Raw data used for survival modeling CoxPHSummary Summary of Cox PH model coefficients WeibullAFTSummary Summary of Weibull AFT model coefficients \ud83d\udc33 Docker Execution The ds service uses a wait-for-etl.sh script to ensure ETL has completed before running. It then launches: python ds_main.py Main entrypoint script: ds_main.py , which calls the full analytics pipeline sequentially.","title":"Model"},{"location":"model/#model-service","text":"The ds service is responsible for performing advanced data analytics, including customer segmentation (via RFM analysis) and customer retention modeling (via survival analysis). It uses preprocessed transaction and customer data from the ETL pipeline and produces insights that are stored in the database and visualized in the frontend.","title":"Model Service"},{"location":"model/#architecture-overview","text":"The model pipeline is built with: SQLAlchemy & pandas : For database interaction and data manipulation scikit-learn : For segmentation (KNN for unknown segment classification) lifelines : For survival analysis (Cox Proportional Hazards, Weibull AFT) matplotlib : For visualization Python + Docker : For containerized execution","title":"\ud83e\uddf1 Architecture Overview"},{"location":"model/#input-output-files","text":"File Path Purpose outputs/customer_transactions.csv Extracted customer transaction data outputs/rfm_results.csv Output of the RFM analysis outputs/survival_data.csv Prepared survival data outputs/*.png Survival plots outputs/*.csv Model summaries","title":"\ud83d\udce6 Input &amp; Output Files"},{"location":"model/#components","text":"","title":"\ud83d\udd0d Components"},{"location":"model/#1-data-extraction","text":"Module: db_ops/extract_and_save.py extract_transaction_data() : Joins FactTransaction, DimCustomer, and DimDate tables for RFM analysis. extract_survival_data() : Constructs a survival dataset with event and duration variables.","title":"1. Data Extraction"},{"location":"model/#2-rfm-analysis","text":"Module: utils/rfm_analyzer.py calculate_rfm() : Computes recency, frequency, and monetary values per customer. score_rfm() : Assigns 1\u20135 scores for each metric. segment_customers() : Classifies customers into segments such as: Champions Loyal Customers Potential Loyalists Big Spenders Leaving Customers classify_unknown_segments() : Uses KNN classifier to assign a segment to previously unclassified customers. analyze_segments() : Produces aggregated statistics per segment. save_results() : Saves the detailed results to CSV.","title":"2. RFM Analysis"},{"location":"model/#3-survival-analysis","text":"Module: utils/survival_analyzer.py Uses lifelines to fit: Cox Proportional Hazards Model Weibull AFT Model Key Methods: fit_cox_model() fit_weibull_model() print_model_summaries() save_model_summaries() plot_weibull_survival_function() plot_custom_profiles()","title":"3. Survival Analysis"},{"location":"model/#output-tables-in-database","text":"Table Description RFMResults RFM scores, segments, and demographic details SurvivalData Raw data used for survival modeling CoxPHSummary Summary of Cox PH model coefficients WeibullAFTSummary Summary of Weibull AFT model coefficients","title":"\ud83e\uddea Output Tables in Database"},{"location":"model/#docker-execution","text":"The ds service uses a wait-for-etl.sh script to ensure ETL has completed before running. It then launches: python ds_main.py Main entrypoint script: ds_main.py , which calls the full analytics pipeline sequentially.","title":"\ud83d\udc33 Docker Execution"}]}