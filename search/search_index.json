{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"api/","text":"","title":"API"},{"location":"app/","text":"","title":"App"},{"location":"database/","text":"Database Documentation This service manages the PostgreSQL database for the ETL pipeline and analytics platform. It uses SQLAlchemy ORM to define and interact with the star schema that powers the reporting and machine learning workflows. \ud83d\udcd0 Star Schema Design The schema follows a classic star structure with the following tables: DimDate : Stores date-related attributes. DimCustomer : Contains customer metadata (e.g., name, birthdate, gender, email). DimStore : Information about store branches including location and size. FactTransaction : The central fact table that records every customer transaction. Each FactTransaction entry references: a customer ( CustomerKey from DimCustomer ), a store ( StoreKey from DimStore ), and a transaction date ( DateKey from DimDate ). \u2699\ufe0f ORM Models The ORM classes in db/star_schema.py define the schema as Python classes. All models inherit from a common Base defined in db_conf.py . The create_tables() function initializes the schema in the target PostgreSQL database. Base.metadata.create_all(bind=engine) \ud83e\udde0 Business Logic Layer The class TransactionDatabase in CRUD_func.py wraps direct database interactions such as: Inserting customers, dates, and transactions Updating or deleting records Fetching data for reporting or debugging \ud83d\udd01 ETL Integration The ETL pipeline loads data into the star schema in multiple phases: Extract & Load Raw XLSX : Excel files are ingested into raw tables using Pandas and to_sql() . Raw file names are cleaned into table names. Transform : Data is cleaned and standardized (e.g., phone/email formatting, gender unification). Invalid records are dropped. Load to Star Schema : Dimension tables are filled using load_dimcustomer_table() and load_dimdate_table() . Transaction data is processed and inserted using load_facttransaction_table() . \ud83e\uddea Data Validations Dates must follow YYYY-MM-DD and be logically consistent. CustomerCardCodes are filtered to 13-character strings. Phone and address fields are validated for structure and length. Store names are mapped to known clean values using STORE_NAME_MAPPING . \ud83d\udee0\ufe0f Dependencies The ETL database logic requires: SQLAlchemy psycopg2-binary pandas loguru dotenv \ud83d\udd10 Configuration Database credentials are loaded from a .env file: DATABASE_URL=postgresql+psycopg2://postgres:postgres@db:5432/maindb DB_USER=postgres DB_HOST=postgres-db DB_PASSWORD=postgres DB_NAME=maindb PGADMIN_EMAIL=admin@admin.com PGADMIN_PASSWORD=admin \ud83d\udcc2 Related Files File Description /db SQLAlchemy Setup: Engine, Base, Session, General Database Configuration CRUD_func.py Class wrapping SQL queries extract_load_raw.py Extracts all XLSX files, does validation and dumps them in the DB transform.py Takes raw tables and does transformation, cleaning and validation load.py Takes clean tables and sequentially loads into DB, row-by-row (FactTransaction, DimCustomer) or by batch main.py Entry point that orchestrates ETL and signals completion","title":"Database"},{"location":"database/#database-documentation","text":"This service manages the PostgreSQL database for the ETL pipeline and analytics platform. It uses SQLAlchemy ORM to define and interact with the star schema that powers the reporting and machine learning workflows.","title":"Database Documentation"},{"location":"database/#star-schema-design","text":"The schema follows a classic star structure with the following tables: DimDate : Stores date-related attributes. DimCustomer : Contains customer metadata (e.g., name, birthdate, gender, email). DimStore : Information about store branches including location and size. FactTransaction : The central fact table that records every customer transaction. Each FactTransaction entry references: a customer ( CustomerKey from DimCustomer ), a store ( StoreKey from DimStore ), and a transaction date ( DateKey from DimDate ).","title":"\ud83d\udcd0 Star Schema Design"},{"location":"database/#orm-models","text":"The ORM classes in db/star_schema.py define the schema as Python classes. All models inherit from a common Base defined in db_conf.py . The create_tables() function initializes the schema in the target PostgreSQL database. Base.metadata.create_all(bind=engine)","title":"\u2699\ufe0f ORM Models"},{"location":"database/#business-logic-layer","text":"The class TransactionDatabase in CRUD_func.py wraps direct database interactions such as: Inserting customers, dates, and transactions Updating or deleting records Fetching data for reporting or debugging","title":"\ud83e\udde0 Business Logic Layer"},{"location":"database/#etl-integration","text":"The ETL pipeline loads data into the star schema in multiple phases: Extract & Load Raw XLSX : Excel files are ingested into raw tables using Pandas and to_sql() . Raw file names are cleaned into table names. Transform : Data is cleaned and standardized (e.g., phone/email formatting, gender unification). Invalid records are dropped. Load to Star Schema : Dimension tables are filled using load_dimcustomer_table() and load_dimdate_table() . Transaction data is processed and inserted using load_facttransaction_table() .","title":"\ud83d\udd01 ETL Integration"},{"location":"database/#data-validations","text":"Dates must follow YYYY-MM-DD and be logically consistent. CustomerCardCodes are filtered to 13-character strings. Phone and address fields are validated for structure and length. Store names are mapped to known clean values using STORE_NAME_MAPPING .","title":"\ud83e\uddea Data Validations"},{"location":"database/#dependencies","text":"The ETL database logic requires: SQLAlchemy psycopg2-binary pandas loguru dotenv","title":"\ud83d\udee0\ufe0f Dependencies"},{"location":"database/#configuration","text":"Database credentials are loaded from a .env file: DATABASE_URL=postgresql+psycopg2://postgres:postgres@db:5432/maindb DB_USER=postgres DB_HOST=postgres-db DB_PASSWORD=postgres DB_NAME=maindb PGADMIN_EMAIL=admin@admin.com PGADMIN_PASSWORD=admin","title":"\ud83d\udd10 Configuration"},{"location":"database/#related-files","text":"File Description /db SQLAlchemy Setup: Engine, Base, Session, General Database Configuration CRUD_func.py Class wrapping SQL queries extract_load_raw.py Extracts all XLSX files, does validation and dumps them in the DB transform.py Takes raw tables and does transformation, cleaning and validation load.py Takes clean tables and sequentially loads into DB, row-by-row (FactTransaction, DimCustomer) or by batch main.py Entry point that orchestrates ETL and signals completion","title":"\ud83d\udcc2 Related Files"},{"location":"model/","text":"Model Service The ds service is responsible for performing advanced data analytics, including customer segmentation (via RFM analysis) and customer retention modeling (via survival analysis). It uses preprocessed transaction and customer data from the ETL pipeline and produces insights that are stored in the database and visualized in the frontend. \ud83e\uddf1 Architecture Overview The model pipeline is built with: SQLAlchemy & pandas : For database interaction and data manipulation scikit-learn : For segmentation (KNN for unknown segment classification) lifelines : For survival analysis (Cox Proportional Hazards, Weibull AFT) matplotlib : For visualization Python + Docker : For containerized execution \ud83d\udce6 Input & Output Files File Path Purpose outputs/customer_transactions.csv Extracted customer transaction data outputs/rfm_results.csv Output of the RFM analysis outputs/survival_data.csv Prepared survival data outputs/*.png Survival plots outputs/*.csv Model summaries \ud83d\udd0d Components 1. Data Extraction Module: db_ops/extract_and_save.py extract_transaction_data() : Joins FactTransaction, DimCustomer, and DimDate tables for RFM analysis. extract_survival_data() : Constructs a survival dataset with event and duration variables. 2. RFM Analysis Module: utils/rfm_analyzer.py calculate_rfm() : Computes recency, frequency, and monetary values per customer. score_rfm() : Assigns 1\u20135 scores for each metric. segment_customers() : Classifies customers into segments such as: Champions Loyal Customers Potential Loyalists Big Spenders Leaving Customers classify_unknown_segments() : Uses KNN classifier to assign a segment to previously unclassified customers. analyze_segments() : Produces aggregated statistics per segment. save_results() : Saves the detailed results to CSV. 3. Survival Analysis Module: utils/survival_analyzer.py Uses lifelines to fit: Cox Proportional Hazards Model Weibull AFT Model Key Methods: fit_cox_model() fit_weibull_model() print_model_summaries() save_model_summaries() plot_weibull_survival_function() plot_custom_profiles() \ud83e\uddea Output Tables in Database Table Description RFMResults RFM scores, segments, and demographic details SurvivalData Raw data used for survival modeling CoxPHSummary Summary of Cox PH model coefficients WeibullAFTSummary Summary of Weibull AFT model coefficients \ud83d\udc33 Docker Execution The ds service uses a wait-for-etl.sh script to ensure ETL has completed before running. It then launches: python ds_main.py Main entrypoint script: ds_main.py , which calls the full analytics pipeline sequentially.","title":"Models"},{"location":"model/#model-service","text":"The ds service is responsible for performing advanced data analytics, including customer segmentation (via RFM analysis) and customer retention modeling (via survival analysis). It uses preprocessed transaction and customer data from the ETL pipeline and produces insights that are stored in the database and visualized in the frontend.","title":"Model Service"},{"location":"model/#architecture-overview","text":"The model pipeline is built with: SQLAlchemy & pandas : For database interaction and data manipulation scikit-learn : For segmentation (KNN for unknown segment classification) lifelines : For survival analysis (Cox Proportional Hazards, Weibull AFT) matplotlib : For visualization Python + Docker : For containerized execution","title":"\ud83e\uddf1 Architecture Overview"},{"location":"model/#input-output-files","text":"File Path Purpose outputs/customer_transactions.csv Extracted customer transaction data outputs/rfm_results.csv Output of the RFM analysis outputs/survival_data.csv Prepared survival data outputs/*.png Survival plots outputs/*.csv Model summaries","title":"\ud83d\udce6 Input &amp; Output Files"},{"location":"model/#components","text":"","title":"\ud83d\udd0d Components"},{"location":"model/#1-data-extraction","text":"Module: db_ops/extract_and_save.py extract_transaction_data() : Joins FactTransaction, DimCustomer, and DimDate tables for RFM analysis. extract_survival_data() : Constructs a survival dataset with event and duration variables.","title":"1. Data Extraction"},{"location":"model/#2-rfm-analysis","text":"Module: utils/rfm_analyzer.py calculate_rfm() : Computes recency, frequency, and monetary values per customer. score_rfm() : Assigns 1\u20135 scores for each metric. segment_customers() : Classifies customers into segments such as: Champions Loyal Customers Potential Loyalists Big Spenders Leaving Customers classify_unknown_segments() : Uses KNN classifier to assign a segment to previously unclassified customers. analyze_segments() : Produces aggregated statistics per segment. save_results() : Saves the detailed results to CSV.","title":"2. RFM Analysis"},{"location":"model/#3-survival-analysis","text":"Module: utils/survival_analyzer.py Uses lifelines to fit: Cox Proportional Hazards Model Weibull AFT Model Key Methods: fit_cox_model() fit_weibull_model() print_model_summaries() save_model_summaries() plot_weibull_survival_function() plot_custom_profiles()","title":"3. Survival Analysis"},{"location":"model/#output-tables-in-database","text":"Table Description RFMResults RFM scores, segments, and demographic details SurvivalData Raw data used for survival modeling CoxPHSummary Summary of Cox PH model coefficients WeibullAFTSummary Summary of Weibull AFT model coefficients","title":"\ud83e\uddea Output Tables in Database"},{"location":"model/#docker-execution","text":"The ds service uses a wait-for-etl.sh script to ensure ETL has completed before running. It then launches: python ds_main.py Main entrypoint script: ds_main.py , which calls the full analytics pipeline sequentially.","title":"\ud83d\udc33 Docker Execution"}]}