{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcca Welcome to Loyalytics Documentation","text":"<p>Loyalytics is a marketing analytics platform developed for a supermarket chain's Bonus Card Loyalty Program. The project addresses declining sales by analyzing customer behavior, segmenting loyalty profiles, and enabling data-driven decision-making.</p>"},{"location":"#problem","title":"\ud83e\udde0 Problem","text":"<p>Despite having a bonus card program, the supermarket has experienced declining sales. The challenge is identifying:</p> <p>\"What factors are contributing to the drop in sales among bonus cardholders, and how can segmentation improve retention?\"</p>"},{"location":"#solution","title":"\u2705 Solution","text":"<p>We implemented an end-to-end analytics platform that:</p> <ul> <li>Extracts, cleans, and loads transactional and customer data (ETL).</li> <li>Builds a star schema database to support analytical workloads.</li> <li>Performs RFM segmentation and survival analysis on customer data.</li> <li>Visualizes insights via a Streamlit dashboard.</li> <li>Deploys services using Docker Compose for full-stack orchestration.</li> </ul>"},{"location":"#expected-outcomes","title":"\ud83c\udfaf Expected Outcomes","text":"<ul> <li>Clear understanding of customer loyalty patterns.</li> <li>Identification of high-value vs. at-risk segments.</li> <li>Actionable insights through survival modeling and segment summaries.</li> <li>A fully reproducible and documented system accessible via:</li> <li>Streamlit UI: <code>localhost:8501</code></li> <li>FastAPI Docs: <code>localhost:8008/docs</code></li> <li>PgAdmin UI: <code>localhost:5050</code></li> <li>GitHub Pages: https://ds-223.github.io/Group-5/</li> </ul>"},{"location":"#documentation-map","title":"\ud83d\udcc1 Documentation Map","text":"<ul> <li><code>api.md</code> \u2013 RESTful API endpoints for predictions and search</li> <li><code>app.md</code> - StreamLit, Visualizations, and Email Campaign</li> <li><code>database.md</code> \u2013 Database schema, ORM models, and raw table loading</li> <li><code>model.md</code> \u2013 ML logic, RFM pipeline, and survival analysis</li> </ul>"},{"location":"api/","title":"\ud83d\ude80 API Documentation","text":"<p>This service exposes the REST API layer for the Customer Loyalty &amp; Analytics system. It powers the dashboard, segmentation visualizations, survival models, and automated email campaigns.</p> <p>Built with FastAPI, the API provides endpoints for querying customer data, analyzing transactions, generating RFM segments, and sending retention emails.</p>"},{"location":"api/#tech-stack","title":"\u2699\ufe0f Tech Stack","text":"<ul> <li>FastAPI \u2013 Lightweight, fast web framework for REST endpoints</li> <li>SQLAlchemy \u2013 ORM for interacting with PostgreSQL</li> <li>Pydantic \u2013 Request validation &amp; API response schemas</li> <li>Lifelines \u2013 Kaplan-Meier survival analysis</li> <li>BackgroundTasks \u2013 Async email dispatching</li> </ul>"},{"location":"api/#customer-endpoints","title":"\ud83d\udc64 Customer Endpoints","text":"<p>Manage customer profiles stored in <code>DimCustomer</code>:</p> <ul> <li><code>POST /customers/</code> \u2013 Create a new customer  </li> <li><code>GET /customers/{id}</code> \u2013 Retrieve customer details  </li> <li><code>PUT /customers/{id}</code> \u2013 Update customer profile  </li> <li><code>DELETE /customers/{id}</code> \u2013 Remove a customer record</li> </ul> <p>Also includes:</p> <ul> <li><code>GET /analytics/customer-count-by-gender</code> \u2013 Gender-wise demographic distribution</li> </ul>"},{"location":"api/#transaction-revenue-analytics","title":"\ud83d\udcb0 Transaction &amp; Revenue Analytics","text":"<p>Explore sales data using joins over the star schema:</p> <ul> <li><code>GET /revenue/monthly</code> \u2013 Monthly revenue totals</li> <li><code>GET /customers/{id}/transactions</code> \u2013 Full transaction history of a customer</li> <li><code>GET /analytics/transactions-by-store-month</code> \u2013 Monthly revenue by store</li> <li><code>GET /analytics/transaction-amount-by-store</code> \u2013 Lifetime revenue by store</li> </ul>"},{"location":"api/#rfm-segmentation","title":"\ud83d\udcca RFM Segmentation","text":"<p>Provides data on Recency-Frequency-Monetary segments:</p> <ul> <li><code>GET /analytics/customers-by-segment/{segment}</code> \u2013 List of customers in a specific segment</li> <li><code>GET /analytics/segment-distribution/{all|male|female}</code> \u2013 Distribution of segments overall or by gender</li> <li><code>GET /analytics/rfm-matrix</code> \u2013 RFM matrix for segment summaries</li> </ul>"},{"location":"api/#email-campaigns","title":"\ud83d\udce7 Email Campaigns","text":"<p>Launch targeted retention campaigns via segment-specific email templates:</p> <ul> <li><code>GET /analytics/segments_for_button</code> \u2013 List of all segments for frontend dropdowns</li> <li><code>POST /campaigns/{segment}</code> \u2013 Asynchronously send emails to all users in a segment using BackgroundTasks</li> </ul> <p>Each template includes personalized subject lines and discount codes. See logic in <code>EmailCampaignManager</code>.</p>"},{"location":"api/#survival-analysis","title":"\ud83d\udcc8 Survival Analysis","text":"<p>Use Kaplan-Meier to visualize customer churn over time:</p> <ul> <li><code>GET /analytics/survival-curve</code> \u2013 Returns survival probability at each time step using <code>SurvivalData</code></li> </ul> <p>Supports visualization of customer retention lifecycle.</p>"},{"location":"api/#example-schema-customercreate","title":"\ud83d\udee0\ufe0f Example Schema: <code>CustomerCreate</code>","text":"<pre><code>{\n  \"CustomerKey\": 123,\n  \"CustomerCardCode\": \"BNS1234567890\",\n  \"Name\": \"Jane Doe\",\n  \"RegistrationDate\": \"2020-01-01T00:00:00\",\n  \"BirthDate\": \"1990-05-10\",\n  \"Gender\": \"Female\",\n  \"Phone\": \"(555) 123-4567\",\n  \"Address\": \"123 Loyalty St.\",\n  \"Email\": \"jane.doe@example.com\"\n}\n</code></pre>"},{"location":"api/#security-config","title":"\ud83d\udd10 Security &amp; Config","text":"<p>Sensitive credentials like DB URLs or Gmail credentials are stored in a <code>.env</code> file. Email logic uses OAuth-friendly app passwords.</p>"},{"location":"api/#related-files","title":"\ud83d\udcc2 Related Files","text":"File Description <code>main.py</code> FastAPI app with all endpoints <code>schema.py</code> Pydantic models for request/response <code>email_utils.py</code> Email template logic and SMTP sending <code>columns.py</code> SQLAlchemy table mappings <code>Dockerfile</code> FastAPI container setup <code>requirements.txt</code> All dependencies (e.g., lifelines, fastapi, sqlalchemy)"},{"location":"api/#testing","title":"\ud83e\uddea Testing","text":"<p>Test endpoints locally using:</p> <ul> <li>Swagger UI: <code>localhost:8000/docs</code> </li> <li>ReDoc: <code>localhost:8000/redoc</code></li> </ul>"},{"location":"api/#status","title":"\u2705 Status","text":"<ul> <li>\ud83d\udcec Email logic tested for all segments</li> <li>\ud83d\udcc8 Survival curve rendered correctly</li> <li>\u2705 Endpoints verified via Swagger</li> <li>\ud83d\udc33 Works with Docker Compose stack</li> </ul> <p>For more information, explore index.md, database.md, and model.md.</p>"},{"location":"database/","title":"Database Documentation","text":"<p>This service manages the PostgreSQL database for the ETL pipeline and analytics platform. It uses SQLAlchemy ORM to define and interact with the star schema that powers the reporting and machine learning workflows.</p>"},{"location":"database/#star-schema-design","title":"\ud83d\udcd0 Star Schema Design","text":"<p>The schema follows a classic star structure with the following tables:</p> <ul> <li><code>DimDate</code>: Stores date-related attributes.</li> <li><code>DimCustomer</code>: Contains customer metadata (e.g., name, birthdate, gender, email).</li> <li><code>DimStore</code>: Information about store branches including location and size.</li> <li><code>FactTransaction</code>: The central fact table that records every customer transaction.</li> </ul> <p>Each <code>FactTransaction</code> entry references:</p> <ul> <li>a customer (<code>CustomerKey</code> from <code>DimCustomer</code>),</li> <li>a store (<code>StoreKey</code> from <code>DimStore</code>),</li> <li>and a transaction date (<code>DateKey</code> from <code>DimDate</code>).</li> </ul>"},{"location":"database/#orm-models","title":"\u2699\ufe0f ORM Models","text":"<p>The ORM classes in <code>db/star_schema.py</code> define the schema as Python classes.</p> <ul> <li>All models inherit from a common <code>Base</code> defined in <code>db_conf.py</code>.</li> <li>The <code>create_tables()</code> function initializes the schema in the target PostgreSQL database.</li> </ul> <pre><code>Base.metadata.create_all(bind=engine)\n</code></pre>"},{"location":"database/#business-logic-layer","title":"\ud83e\udde0 Business Logic Layer","text":"<p>The class <code>TransactionDatabase</code> in <code>CRUD_func.py</code> wraps direct database interactions such as:</p> <ul> <li>Inserting customers, dates, and transactions</li> <li>Updating or deleting records</li> <li>Fetching data for reporting or debugging</li> </ul>"},{"location":"database/#etl-integration","title":"\ud83d\udd01 ETL Integration","text":"<p>The ETL pipeline loads data into the star schema in multiple phases:</p> <ol> <li>Extract &amp; Load Raw XLSX:</li> <li>Excel files are ingested into raw tables using Pandas and <code>to_sql()</code>.</li> <li> <p>Raw file names are cleaned into table names.</p> </li> <li> <p>Transform:</p> </li> <li>Data is cleaned and standardized (e.g., phone/email formatting, gender unification).</li> <li> <p>Invalid records are dropped.</p> </li> <li> <p>Load to Star Schema:</p> </li> <li>Dimension tables are filled using <code>load_dimcustomer_table()</code> and <code>load_dimdate_table()</code>.</li> <li>Transaction data is processed and inserted using <code>load_facttransaction_table()</code>.</li> </ol>"},{"location":"database/#data-validations","title":"\ud83e\uddea Data Validations","text":"<ul> <li>Dates must follow <code>YYYY-MM-DD</code> and be logically consistent.</li> <li>CustomerCardCodes are filtered to 13-character strings.</li> <li>Phone and address fields are validated for structure and length.</li> <li>Store names are mapped to known clean values using <code>STORE_NAME_MAPPING</code>.</li> </ul>"},{"location":"database/#dependencies","title":"\ud83d\udee0\ufe0f Dependencies","text":"<p>The ETL database logic requires:</p> <ul> <li><code>SQLAlchemy</code></li> <li><code>psycopg2-binary</code></li> <li><code>pandas</code></li> <li><code>loguru</code></li> <li><code>dotenv</code></li> </ul>"},{"location":"database/#configuration","title":"\ud83d\udd10 Configuration","text":"<p>Database credentials are loaded from a <code>.env</code> file:</p> <pre><code>DATABASE_URL=postgresql+psycopg2://postgres:postgres@db:5432/maindb\nDB_USER=postgres\nDB_HOST=postgres-db\nDB_PASSWORD=postgres\nDB_NAME=maindb\nPGADMIN_EMAIL=admin@admin.com \nPGADMIN_PASSWORD=admin\n</code></pre>"},{"location":"database/#related-files","title":"\ud83d\udcc2 Related Files","text":"File Description <code>/db</code> SQLAlchemy Setup: Engine, Base, Session, General Database Configuration <code>CRUD_func.py</code> Class wrapping SQL queries <code>extract_load_raw.py</code> Extracts all XLSX files, does validation and dumps them in the DB <code>transform.py</code> Takes raw tables and does transformation, cleaning and validation <code>load.py</code> Takes clean tables and sequentially loads into DB, row-by-row (FactTransaction, DimCustomer) or by batch <code>main.py</code> Entry point that orchestrates ETL and signals completion"},{"location":"model/","title":"Model Service","text":"<p>The <code>ds</code> service is responsible for performing advanced data analytics, including customer segmentation (via RFM analysis) and customer retention modeling (via survival analysis). It uses preprocessed transaction and customer data from the ETL pipeline and produces insights that are stored in the database and visualized in the frontend.</p>"},{"location":"model/#architecture-overview","title":"\ud83e\uddf1 Architecture Overview","text":"<p>The model pipeline is built with:</p> <ul> <li>SQLAlchemy &amp; pandas: For database interaction and data manipulation</li> <li>scikit-learn: For segmentation (KNN for unknown segment classification)</li> <li>lifelines: For survival analysis (Cox Proportional Hazards, Weibull AFT)</li> <li>matplotlib: For visualization</li> <li>Python + Docker: For containerized execution</li> </ul>"},{"location":"model/#input-output-files","title":"\ud83d\udce6 Input &amp; Output Files","text":"File Path Purpose <code>outputs/customer_transactions.csv</code> Extracted customer transaction data <code>outputs/rfm_results.csv</code> Output of the RFM analysis <code>outputs/survival_data.csv</code> Prepared survival data <code>outputs/*.png</code> Survival plots <code>outputs/*.csv</code> Model summaries"},{"location":"model/#components","title":"\ud83d\udd0d Components","text":""},{"location":"model/#1-data-extraction","title":"1. Data Extraction","text":"<p>Module: <code>db_ops/extract_and_save.py</code></p> <ul> <li><code>extract_transaction_data()</code>: Joins FactTransaction, DimCustomer, and DimDate tables for RFM analysis.</li> <li><code>extract_survival_data()</code>: Constructs a survival dataset with event and duration variables.</li> </ul>"},{"location":"model/#2-rfm-analysis","title":"2. RFM Analysis","text":"<p>Module: <code>utils/rfm_analyzer.py</code></p> <ul> <li><code>calculate_rfm()</code>: Computes recency, frequency, and monetary values per customer.</li> <li><code>score_rfm()</code>: Assigns 1\u20135 scores for each metric.</li> <li><code>segment_customers()</code>: Classifies customers into segments such as:</li> <li>Champions</li> <li>Loyal Customers</li> <li>Potential Loyalists</li> <li>Big Spenders</li> <li>Leaving Customers</li> <li><code>classify_unknown_segments()</code>: Uses KNN classifier to assign a segment to previously unclassified customers.</li> <li><code>analyze_segments()</code>: Produces aggregated statistics per segment.</li> <li><code>save_results()</code>: Saves the detailed results to CSV.</li> </ul>"},{"location":"model/#3-survival-analysis","title":"3. Survival Analysis","text":"<p>Module: <code>utils/survival_analyzer.py</code></p> <ul> <li>Uses <code>lifelines</code> to fit:</li> <li>Cox Proportional Hazards Model</li> <li>Weibull AFT Model</li> <li>Key Methods:</li> <li><code>fit_cox_model()</code></li> <li><code>fit_weibull_model()</code></li> <li><code>print_model_summaries()</code></li> <li><code>save_model_summaries()</code></li> <li><code>plot_weibull_survival_function()</code></li> <li><code>plot_custom_profiles()</code></li> </ul>"},{"location":"model/#output-tables-in-database","title":"\ud83e\uddea Output Tables in Database","text":"Table Description <code>RFMResults</code> RFM scores, segments, and demographic details <code>SurvivalData</code> Raw data used for survival modeling <code>CoxPHSummary</code> Summary of Cox PH model coefficients <code>WeibullAFTSummary</code> Summary of Weibull AFT model coefficients"},{"location":"model/#docker-execution","title":"\ud83d\udc33 Docker Execution","text":"<p>The <code>ds</code> service uses a <code>wait-for-etl.sh</code> script to ensure ETL has completed before running. It then launches:</p> <pre><code>python ds_main.py\n</code></pre> <p>Main entrypoint script: <code>ds_main.py</code>, which calls the full analytics pipeline sequentially.</p>"}]}